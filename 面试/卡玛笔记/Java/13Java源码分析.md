# Java 源码学习

## Linklist

### 1. 概述

LinkedList底层数据结构是链表，是实现了List接口和Deque接口的双端链表; 其能够高效的实现插入删除操作, 而且也拥有了队列（简单的队列、双端队列）所拥有的特性。

和ArrayList相比，因为其没有实现RandomAccess，是以下标进行访问元素， 所以对于元素访问不及ArrayList，随机访问元素慢。

### 2. 细节

1. 对于普通的单向链表而言，按照下标查找查找的时间复杂度为O(n)，但是对于双向链表按照下标查找，可以利用其特性，选择从头部查找还是从尾部查找，所以更精确来讲，双向链表按照下标查找是O(n/2)，这应该也是LinkedList使用双向链表实现的一个原因吧，在源码里面也能够发现绝大多数的CRUD的API都是先按照下标找到对节点，再进行后续操作。
2. 默认的插入方式是尾部插入，不过也可以明确指定插入方式（头部插入、中间插入、尾部插入），remove方法同理，有这么多插入、删除的形式，为了就是能够在其基础上，实现栈、队列等数据结构。
3. 具有fail-fast机制。

![./pics/_Linklist_weikunkun.png](https://file1.kamacoder.com/i/bagu/_Linklist_weikunkun.png)



## CopyOnWriteArrayList

### 1. 概述

![img](https://file1.kamacoder.com/i/bagu/_copyOnArryalist_weikunkun.png)



CopyOnWriteArrayList可以理解成是ArrayList的线程安全的版本，内部也是使用数组实现；

每次对数组的修改都完全拷贝一份新的数组来修改，修改后再替换原来的老数组，这样子只阻塞的了写操作，不阻塞读操作，实现读写分离。

有一问题是，其无法保证实时的一致性，只能保证最终的一致性，所以适用于对实时性要求不高，读多写少的场景，譬如黑白名单。

### 2. 部分细节

1. 每次对原数组的修改操作，都先加锁，然后copy一份新的数组，在新数组上做修改
2. 锁使用的是ReentrantLock，独占的不公平锁
3. 整个数组使用volatile修饰保证了可见性，结合锁之后，也就确保了单个api操作的线程安全
4. 内部无size属性，直接通过获取当前数组大小得到对应的元素的个数
5. 需要注意的是，因为写时会复制一个近乎等大小的数组，所以需要考虑内存空间和集合使用的业务场景
6. 看完源码之后，加深了System.arraycopy、Arrays.copyOf的理解

### 3. 相似的思想

使用fork创建子进程，避免了内核直接将整个父进程的虚拟内存的数据（堆、栈、数据段、执行代码段）内容整个复制并分配给子进程。

刚开始是让父子进程共享同一个副本，只有在需要写入的时候，数据才会被复制，从而父子进程才拥有各自的副本，在此之前都属于共享的读。

这样之后，对父进程实际物理内存中页的复制被推迟到发生写入的时候。

并且有的时候，共享的副本可能都不会被写（fork之后，调用exec）。

通过写时复制之后，fork的实际开销就变成了复制父进程的页表和给子进程创建PCB。

## treemap

![img](https://gitee.com/CHENKAIforyou/image-bed/raw/master/imag/_treemap_weikunkun.png)



**概述：**

TreeMap底层基于红黑树实现，可以保证在log(n)的时间复杂度内完成containsKey、get、put、remove操作。

同时因为其由红黑树构成，也就是说明了能够维持内部元素的有序性，关于支持内部元素有序性的集合还有LinkedHashMap。

并没有对红黑树做很深的了解，只是跟着整体的思路，使用AVL树实现了一遍。

**部分细节：**

1. 红黑树相比于AVL树，牺牲了部分平衡性，以换取删除/插入操作时少量的旋转次数，整体来说，性能优于AVL树，但是做了性能测试，发现优化了的AVL树和红黑树相比差不了太多。
2. AVL树为了维护严苛的平衡条件，在破坏了平衡之后（插入、删除），需要执行旋转操作。共分为四种：左单旋、先左后右旋、右单旋、先右后左旋。
3. TreeMap内部无扩容的概念，因为使用的是树的链式存储结构
4. 支持范围查找，查找最近的元素
5. 以为内部是按照key进行排序的，所以不支持key为null
6. 排序依据，根据存放的对象是否实现Comprable接口。若实现了，则依据其自定义的compareTo方法，否者需要自定义外部比较器（Comparator），若是都为实现，则报错。

**相关练习题：**(Leetcode)

- [285. 二叉搜索树中的中序后继节点(plus会员)](https://leetcode-cn.com/problems/inorder-successor-in-bst/)
- [108. 将有序数组转换为二叉搜索树](https://leetcode-cn.com/problems/convert-sorted-array-to-binary-search-tree/)
- [110. 平衡二叉树](https://leetcode-cn.com/problems/balanced-binary-tree/)
- [450. 删除二叉搜索树中的节点](https://leetcode-cn.com/problems/delete-node-in-a-bst/)
- [270. 最接近的二叉搜索树值(plus会员)](https://leetcode-cn.com/problems/closest-binary-search-tree-value/)

## PriorityQueue

![img](https://gitee.com/CHENKAIforyou/image-bed/raw/master/imag/_priorityqueue_weikunkun_01.png)



**概述：**

优先队列，是0个或者多个元素构成的集合，集合中按照某种排序方式（元素自身的权重）进行排序，不保证内部元素整体有序，但是每次弹出的元素的优先级最高/低。

内部的数据结构是堆，因为堆底层的结构是完全二叉树，对于树的存储包含有链式存储或者顺序存储，PriorityQueue使用的是顺序存储，所以使用的是Object[]数组，然后利用完全二叉树的性质，解决父子节点关系问题。

默认实现是小根堆

**部分细节：**

1. 未指定初始化容量大小，默认为11，感觉对于底层使用数组实现的集合，默认大小的规定好要没有啥规律可循
2. 扩容比其他集合多了一步，在数组长度 < 64 时，扩容为原先的两倍+2，超过64时，扩容为原先的1.5倍，同时做了放溢出处理，支持最大元素个数Integer.MAX_VALUE;
3. 删除和插入操作均会破坏当前的堆结果，所以每次都需要调用siftUp、siftDown动态调整
4. 插入操作是插入当前堆的末尾，调用siftUp，自底向上调整
5. 删除操作弹出堆顶元素，然后将堆最后一个元素置于堆顶，调用siftDown，自顶向下调整
6. 同样具有fast-fail机制

**相关题目：**

题目的话，感觉还是根据具体情况使用，对于子序列DP问题也可以引入优先队列来优化暴力搜索的过程

(1) 频率相关问题，结合map使用 (2) TopK问题（海量数据处理）

## HashMap

**概述：**

HashMap从宏观来看是基于Hash表实现的，hash冲突的解决方式是拉链法。

具体实现上，在JDK1.8时有做过优化。引入了红黑树，避免因为大量的冲突导致某个位置链表长度过长，使得某些元素的查询效率变低。

同时将链表插入操作的头插法改为了尾插法，保证了插入顺序的同时，也避免了并发操作下的一些异常（并发操作，肯定要想办法解决线程安全嘛，不然出现异常才是正常现象吧）。同时扩容后的rehash的过程也做了优化。

![img](https://gitee.com/CHENKAIforyou/image-bed/raw/master/imag/_Hashmap_weikunkun_01.png)

**部分细节：**

1. 内部采用Node数组来表示Hash表，数组长度为2的幂次，主要是为了能够通过位运算获取key的索引位置，提升计算的效率
2. 链表长度>8时会进行树化，也就是转变为红黑树。大前提是整个Node数组容量>64。同时当树节点数量小于6时又会变为链表
3. 既然是一个集合类，肯定涉及到动态扩容，HashMap内部的扩容机制，默认是扩容为原数组长度的2倍，其次在初始化是若是没有指定容量大小，直接从0扩容为16。如果初始化时指定了容量大小，则整个Hash表的容量大小为最接近指定容量且大于指定容量的2的幂次
4. 扩容过程也伴随着rehash的过程，1.8之前是扩容后，对所有元素rehash，之后映射到对应的位置。1.8时做了优化，巧用的运用了2次幂的扩展(指长度扩为原来2倍)之后，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置这个特点，避免了rehash的过程，而是直接将key的hash值和oldCap相与，如果为0，则保持原位，如果为1，则放入到原位+oldCap的位置。
5. 内部没有capacity变量，Node数组的容量大小是在扩容时确定的
6. 默认的负载因子为0.75，和Redis中的一样，ThreadLocal的为黄金分割点
7. 对key取hash值，采用的是高地位异或求值，从hash分布的情况来看，离散型更好。
8. Redis中的rehash过程和HashMap1.8之前的实现很像，不过是渐进式的，同时其内部是有新旧两个hash表的，在删除、查找、更新操作时，会操作两个hash表，不过在新增操作时，操作的是新的hash表。

**相关题目：**

1. 用于dfs、递归中的缓存操作，防止超时
2. 用于查找、频率统计和查重，需要明确key和value的分别代表什么
3. 映射关系，譬如判断给定的字符是否属于ABA这种格式